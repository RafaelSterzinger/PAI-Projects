{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "import scipy.signal\n",
    "from gym.spaces import Box, Discrete\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    Compute  cumulative sums of vectors.\n",
    "\n",
    "    Input: [x0, x1, ..., xn]\n",
    "    Output: [x0 + discount * x1 + discount^2 * x2, x1 + discount * x2, ..., xn]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "def combined_shape(length, shape=None):\n",
    "    \"\"\"Helper function that combines two array shapes.\"\"\"\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    \"\"\"The basic multilayer perceptron architecture used.\"\"\"\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPCategoricalActor(nn.Module):\n",
    "    \"\"\"A class for the policy network.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.logits_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        \"\"\"Takes the observation and outputs a distribution over actions.\"\"\"\n",
    "        logits = self.logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        \"\"\"\n",
    "        Take a distribution and action, then gives the log-probability of the action\n",
    "        under that distribution.\n",
    "        \"\"\"\n",
    "        return pi.log_prob(act)\n",
    "\n",
    "    def forward(self, obs, act=None):\n",
    "        \"\"\"\n",
    "        Produce action distributions for given observations, and then compute the\n",
    "        log-likelihood of given actions under those distributions.\n",
    "        \"\"\"\n",
    "        pi = self._distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = self._log_prob_from_distribution(pi, act)\n",
    "        return pi, logp_a\n",
    "    \n",
    "    #Filip Add method\n",
    "    def sample(self, obs):\n",
    "        \n",
    "        pi = self._distribution(obs)\n",
    "        action = pi.sample()\n",
    "        logp_a = self._log_prob_from_distribution(pi, action)\n",
    "        return action, logp_a\n",
    "\n",
    "class MLPCritic(nn.Module):\n",
    "    \"\"\"The network used by the value function.\"\"\"\n",
    "    def __init__(self, obs_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.v_net = mlp([obs_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Critical to ensure v has right shape\n",
    "        return torch.squeeze(self.v_net(obs), -1)\n",
    "\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "    \"\"\"Class to combine policy and value function neural networks.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_sizes=(64,64), activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = 8\n",
    "\n",
    "        # Build policy for 4-dimensional action space\n",
    "        self.pi = MLPCategoricalActor(obs_dim, 4, hidden_sizes, activation)\n",
    "\n",
    "        # Build value function\n",
    "        self.v  = MLPCritic(obs_dim, hidden_sizes, activation)\n",
    "\n",
    "    def step(self, state):\n",
    "        \"\"\"\n",
    "        Take an state and return action, value function, and log-likelihood\n",
    "        of chosen action.\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function.\n",
    "        # It is supposed to return three numbers:\n",
    "        #    1. An action sampled from the policy given a state (0, 1, 2 or 3)\n",
    "        #    2. The value function at the given state\n",
    "        #    3. The log-probability of the action under the policy output distribution\n",
    "        # Hint: This function is only called during inference. You should use\n",
    "        # `torch.no_grad` to ensure that it does not interfer with the gradient computation.\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action, logp_action = self.pi.sample(state)\n",
    "            val_f = self.v.forward(state)\n",
    "        \n",
    "        return action.item(), val_f.item(), logp_action.item()\n",
    "\n",
    "    def act(self, state):\n",
    "        return self.step(state)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPGBuffer:\n",
    "    \"\"\"\n",
    "    Buffer to store trajectories.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma, lam):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        # calculated TD residuals\n",
    "        self.tdres_buf = np.zeros(size, dtype=np.float32)\n",
    "        # rewards\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        # trajectory's remaining return\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        # values predicted\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        # log probabilities of chosen actions under behavior policy\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append a single timestep to the buffer. This is called at each environment\n",
    "        update to store the outcome observed outcome.\n",
    "        \"\"\"\n",
    "        # buffer has to have room so you can store\n",
    "        assert self.ptr < self.max_size\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def end_traj(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call after a trajectory ends. Last value is value(state) if cut-off at a\n",
    "        certain state, or 0 if trajectory ended uninterrupted\n",
    "        \"\"\"\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "\n",
    "        # TODO: Implement TD residual calculation.\n",
    "        # Hint: we do the discounting for you, you just need to compute 'deltas'.\n",
    "        # see the handout for more info\n",
    "        # deltas = rews[:-1] + ...\n",
    "        \n",
    "        deltas = rews[:-1] + vals[1::] - vals[:-1] # can also average val across episodes\n",
    "        self.tdres_buf[path_slice] = discount_cumsum(deltas, self.gamma*self.lam)\n",
    "\n",
    "        #TODO: compute the discounted rewards-to-go. Hint: use the discount_cumsum function\n",
    "        \n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews[:-1], self.gamma)\n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call after an epoch ends. Resets pointers and returns the buffer contents.\n",
    "        \"\"\"\n",
    "        # Buffer has to be full before you can get something from it.\n",
    "        assert self.ptr == self.max_size\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "\n",
    "        # TODO: Normalize the TD-residuals in self.tdres_buf\n",
    "        self.tdres_buf = (self.tdres_buf - np.mean(self.tdres_buf)) / np.std(self.tdres_buf)\n",
    "\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf, tdres=self.tdres_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50, mean return -291.4248993164757\n",
      "Tensor after sum tensor(-1.0382, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 2/50, mean return -349.710548484882\n",
      "Tensor after sum tensor(14.2426, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 3/50, mean return -364.4627727514372\n",
      "Tensor after sum tensor(17.4178, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 4/50, mean return -350.7280488025321\n",
      "Tensor after sum tensor(2.0790, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 5/50, mean return -316.95959686615237\n",
      "Tensor after sum tensor(0.8060, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 6/50, mean return -294.3592022523435\n",
      "Tensor after sum tensor(27.6806, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 7/50, mean return -331.7494288483622\n",
      "Tensor after sum tensor(-2.5947, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 8/50, mean return -296.22113064004924\n",
      "Tensor after sum tensor(-5.9317, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 9/50, mean return -355.39982955923074\n",
      "Tensor after sum tensor(11.9605, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 10/50, mean return -324.5107033652686\n",
      "Tensor after sum tensor(2.2332, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 11/50, mean return -336.7704016718104\n",
      "Tensor after sum tensor(6.8339, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 12/50, mean return -308.67093103185596\n",
      "Tensor after sum tensor(-3.0667, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 13/50, mean return -258.21295560644296\n",
      "Tensor after sum tensor(15.4727, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 14/50, mean return -288.59503184521725\n",
      "Tensor after sum tensor(5.4157, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 15/50, mean return -280.2799199648285\n",
      "Tensor after sum tensor(17.0957, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 16/50, mean return -265.65010420659746\n",
      "Tensor after sum tensor(1.3567, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 17/50, mean return -367.85153383467883\n",
      "Tensor after sum tensor(-5.8653, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 18/50, mean return -310.7592599606074\n",
      "Tensor after sum tensor(17.7459, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 19/50, mean return -312.88745777959474\n",
      "Tensor after sum tensor(0.2185, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 20/50, mean return -326.4331631185138\n",
      "Tensor after sum tensor(10.7830, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 21/50, mean return -260.1586254432658\n",
      "Tensor after sum tensor(0.9975, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 22/50, mean return -329.57920666934945\n",
      "Tensor after sum tensor(19.9396, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 23/50, mean return -355.8324111423307\n",
      "Tensor after sum tensor(11.1547, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 24/50, mean return -311.0048079043539\n",
      "Tensor after sum tensor(3.9595, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 25/50, mean return -317.3633926806915\n",
      "Tensor after sum tensor(3.1606, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 26/50, mean return -309.4668157988407\n",
      "Tensor after sum tensor(-7.9650, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 27/50, mean return -348.46521210537594\n",
      "Tensor after sum tensor(9.1673, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 28/50, mean return -295.76524533835124\n",
      "Tensor after sum tensor(3.8046, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 29/50, mean return -324.9359367183612\n",
      "Tensor after sum tensor(8.7290, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 30/50, mean return -336.97432503897653\n",
      "Tensor after sum tensor(7.5145, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 31/50, mean return -370.8323582954887\n",
      "Tensor after sum tensor(19.9879, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 32/50, mean return -369.3373244276613\n",
      "Tensor after sum tensor(7.8831, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 33/50, mean return -285.1507077670563\n",
      "Tensor after sum tensor(-0.5496, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 34/50, mean return -312.43840778255674\n",
      "Tensor after sum tensor(14.8862, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 35/50, mean return -339.5417247172741\n",
      "Tensor after sum tensor(-4.5469, grad_fn=<SumBackward0>)\n",
      "-----------\n",
      "Epoch: 36/50, mean return -319.5155750446604\n",
      "Tensor after sum tensor(10.3930, grad_fn=<SumBackward0>)\n",
      "-----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-87d5da6fa13b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-94-87d5da6fa13b>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[0mrec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVideoRecorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"policy.mp4\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-87d5da6fa13b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                 \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mac\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogp_a_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mac\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Filip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-5d4b18f758c4>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogp_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[0mval_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-5d4b18f758c4>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mlogp_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_prob_from_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-5d4b18f758c4>\u001b[0m in \u001b[0;36m_distribution\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;34m\"\"\"Takes the observation and outputs a distribution over actions.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\paitask4\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\paitask4\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\paitask4\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\paitask4\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\paitask4\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1675\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1676\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1677\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.hid = 64  # layer width of networks\n",
    "        self.l = 2  # layer number of networks\n",
    "        # initialises an actor critic\n",
    "        self.ac = MLPActorCritic(hidden_sizes=[self.hid]*self.l)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Main training loop.\n",
    "\n",
    "        IMPORTANT: This function called by the checker to train your agent.\n",
    "        You SHOULD NOT change the arguments this function takes and what it outputs!\n",
    "        \"\"\"\n",
    "\n",
    "        # The observations are 8 dimensional vectors, and the actions are numbers,\n",
    "        # i.e. 0-dimensional vectors (hence act_dim is an empty list).\n",
    "        obs_dim = [8]\n",
    "        act_dim = []\n",
    "\n",
    "        # Training parameters\n",
    "        # You may wish to change the following settings for the buffer and training\n",
    "        # Number of training steps per epoch\n",
    "        steps_per_epoch = 3000\n",
    "        # Number of epochs to train for\n",
    "        epochs = 50\n",
    "        # The longest an episode can go on before cutting it off\n",
    "        max_ep_len = 300\n",
    "        # Discount factor for weighting future rewards\n",
    "        gamma = 0.99\n",
    "        lam = 0.97\n",
    "\n",
    "        # Learning rates for policy and value function\n",
    "        pi_lr = 3e-3\n",
    "        vf_lr = 1e-3\n",
    "\n",
    "        # Set up buffer\n",
    "        buf = VPGBuffer(obs_dim, act_dim, steps_per_epoch, gamma, lam)\n",
    "\n",
    "        # Initialize the ADAM optimizer using the parameters\n",
    "        # of the policy and then value networks\n",
    "        # TODO: Use these optimizers later to update the policy and value networks.\n",
    "        pi_optimizer = Adam(self.ac.pi.parameters(), lr=pi_lr)\n",
    "        v_optimizer = Adam(self.ac.v.parameters(), lr=vf_lr)\n",
    "\n",
    "        # Initialize the environment\n",
    "        state, ep_ret, ep_len = self.env.reset(), 0, 0\n",
    "        \n",
    "        ret_sums = 0 #Filip\n",
    "        mse_loss = nn.MSELoss() #Filip\n",
    "\n",
    "        # Main training loop: collect experience in env and update / log each epoch\n",
    "        for epoch in range(epochs):\n",
    "            ep_returns = []\n",
    "            list_logp_a_grad = [] # Filip\n",
    "            list_v_grad = [] # Filip\n",
    "            \n",
    "            for t in range(steps_per_epoch):\n",
    "                \n",
    "                a, v, logp = self.ac.step(torch.as_tensor(state, dtype=torch.float32))\n",
    "                \n",
    "                pi, logp_a_grad = self.ac.pi.forward(torch.as_tensor(state, dtype=torch.float32), torch.tensor(a)) #Filip\n",
    "                list_logp_a_grad.append(logp_a_grad) #Filip\n",
    "                \n",
    "                v_grad = self.ac.v.forward(torch.as_tensor(state, dtype=torch.float32)) #Filip\n",
    "                list_v_grad.append(v_grad) #Filip\n",
    "                \n",
    "                next_state, r, terminal = self.env.transition(a)\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "\n",
    "                # Log transition\n",
    "                buf.store(state, a, r, v, logp)\n",
    "\n",
    "                # Update state (critical!)\n",
    "                state = next_state\n",
    "\n",
    "                timeout = ep_len == max_ep_len\n",
    "                epoch_ended = (t == steps_per_epoch - 1)\n",
    "\n",
    "                if terminal or timeout or epoch_ended:\n",
    "                    # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                    if epoch_ended:\n",
    "                        _, v, _ = self.ac.step(torch.as_tensor(state, dtype=torch.float32))\n",
    "                    else:\n",
    "                        v = 0\n",
    "                    if timeout or terminal:\n",
    "                        ep_returns.append(ep_ret)  # only store return when episode ended\n",
    "                    buf.end_traj(v)\n",
    "                    state, ep_ret, ep_len = self.env.reset(), 0, 0\n",
    "\n",
    "            mean_return = np.mean(ep_returns) if len(ep_returns) > 0 else np.nan\n",
    "            print(f\"Epoch: {epoch+1}/{epochs}, mean return {mean_return}\")\n",
    "\n",
    "            # This is the end of an epoch, so here is where you likely want to update\n",
    "            # the policy and / or value function.\n",
    "            # TODO: Implement the policy and value function update. Hint: some of the torch code is\n",
    "            # done for you.\n",
    "            \n",
    "            data = buf.get()\n",
    "            loss_pi = None\n",
    "            if epoch == 0:\n",
    "                sum_tdres = data['tdres']\n",
    "            else:\n",
    "                sum_tdres += data['tdres']\n",
    "                \n",
    "            tensor_logp_a_grad = torch.tensor(list_logp_a_grad) # dif between tensor and Tensor?\n",
    "            tensor_v_grad = torch.tensor(list_v_grad)\n",
    "            \n",
    "            tensor_logp_a_grad.requires_grad = True\n",
    "            tensor_v_grad.requires_grad = True\n",
    "\n",
    "            pi_optimizer.zero_grad()\n",
    "            if loss_pi is None:\n",
    "                loss_pi = torch.sum(torch.mul(tensor_logp_a_grad, data['tdres']))\n",
    "            else:\n",
    "                loss_pi += torch.sum(torch.mul(tensor_logp_a_grad, data['tdres']))\n",
    "            \n",
    "            loss_pi_new = loss_pi.mul(-1/((epoch+1)*t))\n",
    "            #print(\"Tensor of log probabilities\", tensor_logp_a_grad)\n",
    "            #print(\"Tensor of sum of residuals\", sum_tdres)\n",
    "            #print(\"Tensor after multiplication\", )\n",
    "            print(\"Tensor after sum\", loss_pi)\n",
    "            print(\"-----------\")\n",
    "            \n",
    "            loss_pi_new.backward()\n",
    "            pi_optimizer.step()\n",
    "            \n",
    "            for _ in range(100):\n",
    "                \n",
    "                v_optimizer.zero_grad()\n",
    "                loss_v = mse_loss(tensor_v_grad, data['ret'])\n",
    "                loss_v.backward()\n",
    "                v_optimizer.step()\n",
    "\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        \"\"\"\n",
    "        Sample an action from your policy.\n",
    "\n",
    "        IMPORTANT: This function called by the checker to evaluate your agent.\n",
    "        You SHOULD NOT change the arguments this function takes and what it outputs!\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function.\n",
    "\n",
    "        return self.ac.act(obs)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Train and evaluate agent.\n",
    "\n",
    "    This function basically does the same as the checker that evaluates your agent.\n",
    "    You can use it for debugging your agent and visualizing what it does.\n",
    "    \"\"\"\n",
    "    from lunar_lander import LunarLander\n",
    "    from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "    env = LunarLander()\n",
    "\n",
    "    agent = Agent(env)\n",
    "    agent.train()\n",
    "\n",
    "    rec = VideoRecorder(env, \"policy.mp4\")\n",
    "    episode_length = 300\n",
    "    n_eval = 100\n",
    "    returns = []\n",
    "    print(\"Evaluating agent...\")\n",
    "\n",
    "    for i in range(n_eval):\n",
    "        print(f\"Testing policy: episode {i+1}/{n_eval}\")\n",
    "        state = env.reset()\n",
    "        cumulative_return = 0\n",
    "        # The environment will set terminal to True if an episode is done.\n",
    "        terminal = False\n",
    "        env.reset()\n",
    "        for t in range(episode_length):\n",
    "            if i <= 10:\n",
    "                rec.capture_frame()\n",
    "            # Taking an action in the environment\n",
    "            action = agent.get_action(state)\n",
    "            state, reward, terminal = env.transition(action)\n",
    "            cumulative_return += reward\n",
    "            if terminal:\n",
    "                break\n",
    "        returns.append(cumulative_return)\n",
    "        print(f\"Achieved {cumulative_return:.2f} return.\")\n",
    "        if i == 10:\n",
    "            rec.close()\n",
    "            print(\"Saved video of 10 episodes to 'policy.mp4'.\")\n",
    "    env.close()\n",
    "    print(f\"Average return: {np.mean(returns):.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5000, 3.5000, 4.5000])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [torch.tensor(1), torch.tensor(2)]\n",
    "x = Variable(torch.tensor(a), requires_grad = True)\n",
    "\n",
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5,6])\n",
    "(a+b) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 5., 3.])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([1,2,3])\n",
    "discount_cumsum(A, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
